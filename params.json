{
  "name": "GitHub Pages",
  "tagline": "by Wuheng Luo",
  "body": "#\r\nRun Machine Intelligence as AlphaGo Does: TensorFlow on Single Machine, in Cluster or Cloud\r\n<br>\r\nby Wuheng Luo \r\n### \r\n05.18.2016\r\n<p>\r\n<p>\r\nToward the end of April this year, about one month and a half after the now famous AlphaGo defeated Go world champion Lee Sedol, AlphaGo’s creator Google DeepMind announced that the team would switch their machine learning framework from Torch7 to TensorFlow for all their future research (see: [\"DeepMind moves to TensorFlow\"](http://googleresearch.blogspot.com/2016/04/deepmind-moves-to-tensorflow.html)).  Though we did not know which convolutional neural network framework AlphaGo used to win the matches against the top human player, now we are pretty sure that when AlphaGo takes on any future machine intelligence endeavors, it will be based on Google’s own TensorFlow.    \r\n<p> \r\nAlphaGo’s historic victory reminds us of 1997 IBM Deep Blue’s triumph over chess world champion Garry Kasparov; but the two are quite different in at least two aspects.  First, Deep Blue is a special-purpose expert system built exclusively for playing chess; whereas AlphaGo is a more general-purpose learning system which can, and reportedly will, undertake other AI tasks in addition to playing Go.  Second, Deep Blue is all about brute-force computation, not really AI per se, or in IBM’s own words, it [“relies more on computational power and a simpler search and evaluation function” ](https://www.research.ibm.com/deepblue/meet/html/d.3.3a.shtml)); whereas AlphaGo’s core components of technology such as convolutional neural network and reinforcement learning are currently very hot areas of machine intelligence.  \r\n<p>\r\nThe good news is, TensorFlow as Google’s second generation machine intelligence framework is already open sourced.  Now everyone can run machine learning as AlphaGo does, using the same deep neural network models.  Here I would like to mention three ways, on a single machine, distributed in a cluster, or in the cloud, that you can quickly set up a TensorFlow environment to train AI models and run your own machine intelligence applications.\r\n<p>\r\nAn easy way to run TensorFlow is to do it on a single machine.  There is a common scenario for data scientists or machine learning practitioners who need to get started in a speedy way without spending too much time on the environment, so that they can work immediately on their algorithms and models with resources and data available.  For that, single machine mode would be a good choice.  When Google initially released TensorFlow, it provided several installation options on a single machine.  My favorite is the Docker approach.  Through Docker containerization, TensorFlow can be set up fast, clean, and isolated from other applications on your machine, even with “batteries” included, that is, with all required dependencies resolved.  To take this route, you need a Docker enabled machine, either a Mac or a PC.  If your machine is not Docker friendly yet, go to the official Docker site to install Docker using the Docker Toolbox.  When docker is installed successfully and you are able to issue “docker” or “sudo docker” command, it is time to pull a recent TensorFlow Docker image from the Google site.  Issue the command below to pull the up to date TensorFlow image from Docker’s image repository Docker Hub:\r\n<p>\r\n``\r\n    $ docker pull gcr.io/tensorflow/tensorflow\r\n``\r\n<p>\r\nThis will download the latest TensorFlow CPU version to your machine.  This image internally includes Python Jupyter notebook, so you can start TensorFlow with Jupyter using the command:\r\n<p>\r\n``\r\n    $ docker run –p 8888:8888 –v <your_local_file_location_path>:/notebook gcr.io/tensorflow/tensorflow sh –c “jupyter notebook /notebook”\r\n``\r\n<p>\r\nThe command above with parameters “sh –c” will launch TensorFlow with Jupyter started at the notebook doc root /notebook internally inside the TensorFlow instance.  The Docker command option “-v” provides a mechanism to map an external location of your local file system to this internal doc root path.  That way, if you have existing notebook .ipynb files in your file location specified, they will be listed on the homepage under “Files” when TensorFlow is started with Jupyter notebook.  The benefit of using –v option to map an external local file location to the container is that the user can start, stop, destroy, or recycle a TensorFlow instance at will without losing any coding artifact.  One thing to remember for the execution command above is to replace <your_local_file_location_path> with your own actual file location.\r\n<p> \r\nAnother way to run TensorFlow is to do it in distributed mode on a cluster rather than on a single machine.  For that a good option is to use Google Cloud Datalab, where you can attach a TensorFlow enabled notebook to a cluster on Google Cloud Platform, and run distributed deep neural network models there.  As of this writing, Google Cloud Datalab is still in beta version.  A Google account is required to proceed.  If you already have an account and also have a cluster provisioned, go to https://cloud.google.com/datalab/ to launch the datalab.  After signed in, you need to select an existing cloud project or create a new one, in which to deploy the datalab.  Once the deployment is done, you will see the Datalab Notebook home page, where you can create new notebooks or select saved notebooks to work on, and upload your data, etc.  Datalab has a version of TensorFlow pre-installed.  You can open a new notebook and test that by entering the Python code “import tensorflow as tf” and press Shift+Enter keys on your keyboard to run the command.  If it does not return any error, you are ready to run your TensorFlow jobs, at least in single machine mode.\r\n<p>  \r\nIf you want to run TensorFlow in a cluster’s distributed mode, you need to run one more test.  Enter this code block in your Datalab notebook to see whether that gives you any error:\r\n<p>\r\n    import tensorflow as tf<br><br>\r\n    c = tf.constant(\"Hello, distributed TensorFlow!\")<br>\r\n    server = tf.train.Server.create_local_server()<br>\r\n    sess = tf.Session(server.target)  # Create a session on the server.<br>\r\n    sess.run(c)<br>\r\n<p>\r\nLine 3 of the code, “tf.train.Server.create_local_server()”, is the syntax only for distributed TensorFlow.  If that line returns an error: “AttributeError: 'module' object has no attribute 'Server'”, your pre-installed TensorFlow version needs an upgrade.  You can do the upgrade by yourself with a few steps.  First, go to the TensorFlow official site, https://www.tensorflow.org/ and click on one of the top level menus such as “API” to find out the latest version, indicated on top of the left navigation panel.  Second, in your notebook, enable bash shell mode by entering the imperative directive: “%%bash” in order to run a re-installation.  Third, enter the command below to upgrade (make sure to replace <version#> with the version you found in step 1, e.g. 0.8.0.  \r\n<p>\r\n``\r\n%%bash\r\npip install protobuf==3.0.0b2.post2 && \\\r\n    wget https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-<version#>-cp27-none-linux_x86_64.whl && \\\r\n    pip install tensorflow-<version#>-cp27-none-linux_x86_64.whl && \\\r\n    rm tensorflow-<version#>-cp27-none-linux_x86_64.whl\r\n``\r\n<P>\r\nThe notebook will display installation log messages, and if things go well, you will see log info:  “Successfully installed tensorflow protobuf ...”.  To test the upgraded TensorFlow version, make sure you open a new notebook, not in the existing notebook where you issued “pip install” command, since that one is already attached to the cluster with original configuration in cache.  In a new notebook, run the code block above that contains the line “server = tf.train.Server.create_local_server()”.  This time, if the execution does not generate errors, you will see a message returned: “'Hello, distributed TensorFlow!'”.  Now you are ready to explore and enjoy the power of distributed deep neural network frameworks as AlphaGo does.\r\n\r\nThere is one more approach to running TensorFlow in a cluster of distributed nodes, that is, through Apache Spark.  Spark vendor Databricks published a technical blog on TensorFlow performance in Spark, and their benchmarks showed “34% lower error rate” and “7x speedup” in performance using a cluster of 13 nodes (vs. a single machine).  A quick way to set up a TensorFlow enabled Spark environment is to go to the cloud, and Databricks recently released their beta Databricks Community Edition (DCE).  DCE currently has a Spark 2.0 preview available for their notebooks, so if you have a DCE account, you are able to try forthcoming latest Spark features.  To try TensorFlow with Spark combination on DCE, the first thing to do after logging in, is to install TensorFlow, since Apache Spark distribution does not include TensorFlow by default (verify this by running the import command on a DCE notebook, “import tensorflow as tf”, and it shall return an error: “ImportError: No module named tensorflow”).  A quick way to install is to use the %sh directive on the notebook, again make sure to replace <version#> in the command with the latest TensorFlow version number: \r\n<p>\r\n``\r\n%sh\r\nsudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-<version#>-cp27-none-linux_x86_64.whl  \r\n``             \r\n<p>\r\nA more elegant way to install, also on the notebook, is to use Python “try … except” clauses to cover possible scenarios (again, remember to replace <version#>):\r\n<p>\r\n\r\ntry:<br>\r\n    import tensorflow as tf<br>\r\n    print \"TensorFlow is already installed\"<br>\r\nexcept ImportError:<br>\r\n    print \"Installing TensorFlow\"<br>\r\n    import subprocess<br>\r\n    subprocess.check_call([\"/databricks/python/bin/pip\", \"install\",<br> \r\n        \"https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-<version#>-cp27-none-linux_x86_64.whl\"])<br>\r\n<p>\r\nAfter the installation is done, you also need to make sure the installed framework is available.  Rather than open a new notebook as mentioned before with Google Datalab, on the Databricks Spark notebook, click the dropdown on top left corner, do “detach” the cluster, then “attach” the cluster one more time to enable TensorFlow just installed.  Now you are ready to run distributed TensorFlow jobs on your notebook.  Databricks has a sample Spark notebook source code, “Distributed processing of images using TensorFlow”, available [here](http://go.databricks.com/hubfs/notebooks/TensorFlow/Distributed_processing_of_images_using_TensorFlow.html), so give it a try.\r\n\r\n  ",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}